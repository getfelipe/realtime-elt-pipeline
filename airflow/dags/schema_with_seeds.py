"""
Create table schemas based on the CSV files generated by the producer and stored in the volume /opt/airflow/dags/dbt_2/seeds/.
"""

from airflow import DAG
from datetime import datetime
from airflow.operators.bash import BashOperator
from airflow.operators.python import ShortCircuitOperator
from airflow.operators.python import BranchPythonOperator
from airflow.operators.dummy import DummyOperator
import os
import sys
sys.path.insert(0, '/usr/local/airflow/') # Import the modules from the path
from include.profiles import project_path, profiles_dir
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.trigger_rule import TriggerRule

dir = "/opt/airflow/dags/dbt_2/seeds/"
files = ['machine_list.csv', 'operator_list.csv', 'status_table.csv', 'production_list.csv']
machine_path = os.path.join(dir, files[0])
operator_path = os.path.join(dir, files[1])
status_path = os.path.join(dir, files[2])
production_path = os.path.join(dir, files[3])
path_manifest_local = "/usr/local/airflow/dags/dbt_2/target/"
current_conn = "postgres_default" #"postgres_prod"

with DAG(
    dag_id='schema_with_seeds',
    start_date=None,
    schedule_interval=None,
    catchup=False,
) as dag:
    
    # Function to check if the tables exist and have data
    def check_tables():
        tables = ['machine_list', 'status_table', 'operator_list']
        pg_hook = PostgresHook(postgres_conn_id=current_conn)
        check = []
        for table in tables:
            sql = f"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table}' AND table_schema = 'maintenance');"
            exists = pg_hook.get_first(sql)[0]
            if not exists:
                print(f"Table {table} does not exist.")
                check.append(True)
                continue

            sql = f"SELECT EXISTS (SELECT 1 FROM maintenance.\"{table}\" LIMIT 1);"
            has_data = pg_hook.get_first(sql)[0]
            if has_data:
                print(f"Table {table} exists and has registers.")
                check.append(False)
            else:
                print(f"Table {table} exists and has not registers.")
                check.append(True)

        if any(check):
            task = 'task_first_load'

        else:
            task = 'task_second_load'

        return task

    # Function to check if the files exist
    def condition():
        return (
            os.path.isfile(machine_path) and
            os.path.isfile(operator_path) and
            os.path.isfile(status_path)
        )

    branch_task = BranchPythonOperator(
        task_id='branch_task',
        python_callable=check_tables,
    )

    dbt_debug = BashOperator(
       task_id="dbt_debug",
       bash_command=f"source /usr/local/airflow/dbt_venv/bin/activate && dbt debug --target dev --project-dir {project_path} --profiles-dir {profiles_dir}",
    )

    # Create schema
    run_bash_task = BashOperator(
        task_id='run_bash_task',
        bash_command=f"source /usr/local/airflow/dbt_venv/bin/activate && dbt seed --target dev --target-path {path_manifest_local} --project-dir {project_path} --profiles-dir {profiles_dir}"
    )

    task_first_load = DummyOperator(
            task_id='task_first_load'
        )
    task_second_load = DummyOperator(
            task_id='task_second_load'
        )

    trigger_load_to_postgres_first = TriggerDagRunOperator(
        task_id="trigger_load_to_postgres_first",
        trigger_dag_id="load_to_postgres",
        #trigger_rule=TriggerRule.ALL_SUCCESS,
        conf={"first_load": True}, # first_load = True when the DAG is triggered for the first time
    )

    trigger_load_to_postgres_second = TriggerDagRunOperator(
        task_id="trigger_load_to_postgres_second",
        trigger_dag_id="load_to_postgres",
        #trigger_rule=TriggerRule.ALL_SUCCESS,
        conf={"first_load": False},
    )

    # Define the task dependencies
    branch_task >> [task_first_load, task_second_load]
    task_first_load >> dbt_debug >> run_bash_task >> trigger_load_to_postgres_first
    task_second_load >> trigger_load_to_postgres_second


